{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2b95e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Импорты ---\n",
    "import os\n",
    "import datetime\n",
    "import multiprocessing\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio.v2 as imageio\n",
    "import requests\n",
    "\n",
    "import torchvision\n",
    "import torchvision.utils as vutils\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from vae_v2 import VAE, VGGPerceptualLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc025b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Гиперпараметры обучения и модели ---\n",
    "BATCH_SIZE = 64\n",
    "# Размер пакета данных для одной итерации обучения.\n",
    "# Большее значение ускоряет обучение, но требует больше видеопамяти.\n",
    "LATENT_DIM = 128 * 4\n",
    "# Размерность латентного пространства (вектора z).\n",
    "# Чем больше размерность, тем богаче потенциальная вариативность генерируемых изображений.\n",
    "IMAGE_SIZE = 128 * 2\n",
    "# Размер входных изображений (высота и ширина в пикселях).\n",
    "# Должен быть кратным 16 для корректной работы архитектуры энкодера и декодера.\n",
    "EPOCHS = 500\n",
    "EARLY_STOP = 20 #30\n",
    "# Общее количество проходов по всему датасету во время обучения.\n",
    "LEARNING_RATE = 1e-3\n",
    "# Скорость обучения оптимизатора.\n",
    "# Малое значение приводит к медленному обучению, большое может вызывать нестабильность.\n",
    "\n",
    "SAVE_INTERVAL = 100\n",
    "# Интервал в эпохах для сохранения сгенерированных изображений в процессе обучения.\n",
    "\n",
    "SAVE_PATH = './models/vae_model.pth'\n",
    "# Путь для сохранения финальной версии обученной модели.\n",
    "BEST_MODEL_PATH = './models/vae_best_model.pth'\n",
    "# Путь для сохранения лучшей модели (по минимальному значению лосса на обучении).\n",
    "OLD_MODEL = False\n",
    "# Путь к ранее обученной модели для продолжения обучения или генерации изображений.\n",
    "# False означает обучение \\\"с нуля\\\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be2ccbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Folders\n",
    "OUTPUT = './output'\n",
    "EPOCHS_FOLDER = os.path.join(OUTPUT, 'epochs')\n",
    "GENERATED_FOLDER = os.path.join(OUTPUT, 'generated')\n",
    "MORPH_FOLDER = os.path.join(OUTPUT, 'morph_frames')\n",
    "Z_FOLDER = os.path.join(OUTPUT, 'z_frames')\n",
    "LAT_GRID = os.path.join(OUTPUT, 'latent_frames')\n",
    "GRID_FOLDER = os.path.join(OUTPUT, 'grid')\n",
    "os.makedirs(OUTPUT, exist_ok=True)\n",
    "os.makedirs(EPOCHS_FOLDER, exist_ok=True)\n",
    "os.makedirs(GENERATED_FOLDER, exist_ok=True)\n",
    "os.makedirs(MORPH_FOLDER, exist_ok=True)\n",
    "os.makedirs(Z_FOLDER, exist_ok=True)\n",
    "os.makedirs(LAT_GRID, exist_ok=True)\n",
    "os.makedirs(GRID_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a98e16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using DEVICE: {DEVICE}\")\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13edddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подготовка данных\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabaf893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание загрузчиков данных\n",
    "num_workers = min(16, multiprocessing.cpu_count())\n",
    "prefetch_factor = 128#* 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e496c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.ImageFolder(root='data', transform=transform)\n",
    "#dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,           # обязательно перемешивание\n",
    "    num_workers=num_workers,           # или другое оптимальное число\n",
    "    pin_memory=True,\n",
    "    prefetch_factor=prefetch_factor,       # если много CPU\n",
    "    persistent_workers=True  # если поддерживается версией\n",
    ")\n",
    "\n",
    "print(f\"Loaded {len(dataset)} images from 'data'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19a8f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализируем Perceptual Loss\n",
    "perceptual_loss_fn = VGGPerceptualLoss().to(DEVICE)\n",
    "\n",
    "def vae_loss_function(recon_x, x, mu, logvar, alpha=1.0, beta=0.1):\n",
    "    recon_loss = nn.functional.mse_loss(recon_x, x, reduction='sum')\n",
    "    perceptual_loss = perceptual_loss_fn(recon_x, x) * x.size(0)  # Масштабируем под батч\n",
    "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    \n",
    "    total_loss = alpha * recon_loss + beta * perceptual_loss + kl_loss\n",
    "    return total_loss\n",
    "\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    recon_loss = nn.functional.mse_loss(recon_x, x, reduction='sum')\n",
    "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return recon_loss + kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d065a6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вспомогательные функции\n",
    "def plot_loss(losses):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(losses, label='Training Loss')\n",
    "    plt.xlabel('Эпоха')\n",
    "    plt.ylabel('Потери')\n",
    "    plt.title('График изменения потерь')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def imshow(img):\n",
    "    img = np.array(img)\n",
    "    if img.min() < 0:\n",
    "        img -= img.min()\n",
    "        img /= img.max()\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(np.transpose(img, (1, 2, 0)))\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb4f93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализация модели\n",
    "vae = VAE(LATENT_DIM, IMAGE_SIZE).to(DEVICE)\n",
    "if OLD_MODEL:\n",
    "    vae.load_state_dict(torch.load(OLD_MODEL, map_location=DEVICE))\n",
    "    print(\"Loaded model from\", OLD_MODEL)\n",
    "optimizer = optim.Adam(vae.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a952c703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучение модели\n",
    "losses = []\n",
    "best_loss = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26205e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    tstart = datetime.datetime.now()\n",
    "    vae.train()\n",
    "    train_loss = 0\n",
    "    for imgs, _ in dataloader:\n",
    "        imgs = imgs.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        recon_imgs, mu, logvar = vae(imgs)\n",
    "        #loss = loss_function(recon_imgs, imgs, mu, logvar)\n",
    "        loss = vae_loss_function(recon_imgs, imgs, mu, logvar)\n",
    "        #loss = perceptual_loss_fn(recon_x, x) * batch_size \n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    avg_loss = train_loss / len(dataset)\n",
    "    losses.append(avg_loss)\n",
    "\n",
    "    print(f\"\\nEpoch [{epoch + 1}/{EPOCHS}], Train Loss: {avg_loss:.4f}\")\n",
    "    print(f\"Время: {str(datetime.datetime.now() - tstart).split('.')[0]}\")\n",
    "    # Сохраняем лучшую модель\n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        best_model = vae\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"val_loss: {avg_loss:.4f} > best_loss: {best_loss:.4f}, попытка {patience_counter}\")\n",
    "        if patience_counter >= EARLY_STOP:\n",
    "            print(\"Ранняя остановка!\")\n",
    "        break\n",
    "    # Периодически генерируем изображения\n",
    "    if (epoch + 1) % SAVE_INTERVAL == 0:\n",
    "        vae.eval()\n",
    "        with torch.no_grad():\n",
    "            z = torch.randn(4, LATENT_DIM).to(DEVICE)\n",
    "            samples = vae.decoder(z)\n",
    "        vutils.save_image(samples.cpu(), f'{EPOCHS_FOLDER}/generated_faces_epoch_{epoch+1}.png', nrow=4)\n",
    "        #imshow(torchvision.utils.make_grid(samples.cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac90b33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BESTLoss = 250\n",
    "best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4982ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохранение финальной модели\n",
    "if best_loss <= BESTLoss:\n",
    "    torch.save(best_model.state_dict(), SAVE_PATH)\n",
    "    print(f\"Final model saved to {SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30de4091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Генерация итоговых лиц\n",
    "vae.eval()\n",
    "with torch.no_grad():\n",
    "    z = torch.randn(16, LATENT_DIM).to(DEVICE)\n",
    "    samples = vae.decoder(z)\n",
    "vutils.save_image(samples.cpu(), f'{GENERATED_FOLDER}/generated_faces_final.png', nrow=4)\n",
    "imshow(torchvision.utils.make_grid(samples.cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb82ced1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Построение графика потерь\n",
    "plot_loss(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46831f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Уведомление о завершении обучения\n",
    "url = \"http://home.teyhd.ru:3334/\"\n",
    "params = {\"msg\": f\"Обучение Автоэнкодера завершено: Эпох {EPOCHS}\\nФинальный лосс: {losses[-1]:.6f}\"}\n",
    "try:\n",
    "    response = requests.get(url, params=params)\n",
    "    print(f\"Notification sent! Status Code: {response.status_code} | Response Text: {response.text}\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to send notification: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12a4162",
   "metadata": {},
   "source": [
    "### Генерация разных типов z для декодера и создание GIF морфинга"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39aa048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Настройки ---\n",
    "NUM_PIC = 16  # Количество изображений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e35ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Стандартное сэмплирование (N(0,1)) ---\n",
    "with torch.no_grad():\n",
    "    z = torch.randn(NUM_PIC, LATENT_DIM).to(DEVICE)\n",
    "    samples = vae.decoder(z)\n",
    "    vutils.save_image(samples.cpu(), f'{Z_FOLDER}/z_random_normal.png', nrow=4)\n",
    "    imshow(torchvision.utils.make_grid(samples.cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b879f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Ручное задание признаков ---\n",
    "with torch.no_grad():\n",
    "    z = torch.zeros(NUM_PIC, LATENT_DIM).to(DEVICE)\n",
    "    z[:, :2] = torch.linspace(-3, 3, NUM_PIC).unsqueeze(1).repeat(1, 2)\n",
    "    samples = vae.decoder(z)\n",
    "    vutils.save_image(samples.cpu(), f'{Z_FOLDER}/z_manual_features.png', nrow=4)\n",
    "    imshow(torchvision.utils.make_grid(samples.cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7489685a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Латентные коды реальных изображений ---\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe195ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_dataset = datasets.ImageFolder(root='data', transform=transform)\n",
    "real_loader = DataLoader(real_dataset, batch_size=NUM_PIC, shuffle=True)\n",
    "\n",
    "real_imgs, _ = next(iter(real_loader))\n",
    "real_imgs = real_imgs.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d56475",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    mu, logvar = vae.encoder(real_imgs)\n",
    "    z = vae.reparameterize(mu, logvar)\n",
    "    samples = vae.decoder(z)\n",
    "    vutils.save_image(samples.cpu(), f'{Z_FOLDER}/z_from_real_imgs.png', nrow=4)\n",
    "    imshow(torchvision.utils.make_grid(samples.cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a38f870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Интерполяция между двумя точками ---\n",
    "frames = []\n",
    "with torch.no_grad():\n",
    "    z1 = torch.randn(1, LATENT_DIM).to(DEVICE)\n",
    "    z2 = torch.randn(1, LATENT_DIM).to(DEVICE)\n",
    "    alphas = torch.linspace(0, 1, steps=30).unsqueeze(1).to(DEVICE)\n",
    "    z = (1 - alphas) * z1 + alphas * z2\n",
    "    for i, zi in enumerate(z):\n",
    "        sample = vae.decoder(zi.unsqueeze(0))\n",
    "        img_path = f'{MORPH_FOLDER}/frame_{i:03d}.png'\n",
    "        vutils.save_image(sample.cpu(), img_path, nrow=1)\n",
    "        frames.append(imageio.imread(img_path))\n",
    "    imageio.mimsave(f'{MORPH_FOLDER}/morphing.gif', frames, fps=10)\n",
    "    print(\"GIF морфинга сохранён как 'morphing.gif'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd05ebf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Показываем последний батч интерполяции\n",
    "vutils.save_image(vae.decoder(z).cpu(), f'{Z_FOLDER}/z_interpolation.png', nrow=6)\n",
    "imshow(torchvision.utils.make_grid(vae.decoder(z).cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5b0292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Альтернативное распределение (равномерное) ---\n",
    "with torch.no_grad():\n",
    "    z = torch.empty(NUM_PIC, LATENT_DIM).uniform_(-2, 2).to(DEVICE)\n",
    "    samples = vae.decoder(z)\n",
    "    vutils.save_image(samples.cpu(), f'{Z_FOLDER}/z_uniform_distribution.png', nrow=4)\n",
    "    imshow(torchvision.utils.make_grid(samples.cpu()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2625482a",
   "metadata": {},
   "source": [
    "### Код для удобного подбора ручных признаков и генерации 2D-сетки лиц + анимация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b4bad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Настройки ---\n",
    "GRID_SIZE = 8  # Сетка GRID_SIZE x GRID_SIZE\n",
    "# Предполагаем, что модель VAE уже загружена и называется \"vae\"\n",
    "vae.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262e04bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Удобный подбор параметров ---\n",
    "def generate_manual_z(fixed_value=0.0, vary_dims=(0, 1), range_min=-3, range_max=3, grid_size=GRID_SIZE):\n",
    "    total = grid_size * grid_size\n",
    "    z = torch.full((total, LATENT_DIM), fixed_value).to(DEVICE)\n",
    "    values = torch.linspace(range_min, range_max, steps=grid_size)\n",
    "    grid = torch.cartesian_prod(values, values)\n",
    "    for idx, (val_x, val_y) in enumerate(grid):\n",
    "        z[idx, vary_dims[0]] = val_x\n",
    "        z[idx, vary_dims[1]] = val_y\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a498b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Генерация 2D-сетки лиц ---\n",
    "def generate_face_grid(z, save_path=f'{GRID_FOLDER}/grid_faces.png', grid_size=GRID_SIZE):\n",
    "    with torch.no_grad():\n",
    "        samples = vae.decoder(z)\n",
    "    vutils.save_image(samples.cpu(), save_path, nrow=grid_size)\n",
    "    print(f\"Сетка лиц сохранена в {save_path}\")\n",
    "    imshow(torchvision.utils.make_grid(samples.cpu(), nrow=grid_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a3ed4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Анимация перемещения по латентному пространству ---\n",
    "def generate_latent_animation(vary_dims=(0, 1), steps=30, fixed_value=0.0, range_min=-3, range_max=3, save_path='{OUTPUT}/latent_walk.gif'):\n",
    "    frames = []\n",
    "  \n",
    "    with torch.no_grad():\n",
    "        for step in range(steps):\n",
    "            alpha = step / (steps - 1)\n",
    "            val_x = (1 - alpha) * range_min + alpha * range_max\n",
    "            val_y = np.sin(alpha * np.pi * 2) * (range_max / 2)\n",
    "\n",
    "            z = torch.full((1, LATENT_DIM), fixed_value).to(DEVICE)\n",
    "            z[0, vary_dims[0]] = val_x\n",
    "            z[0, vary_dims[1]] = val_y\n",
    "\n",
    "            sample = vae.decoder(z)\n",
    "            img_path = f'{LAT_GRID}/frame_{step:03d}.png'\n",
    "            vutils.save_image(sample.cpu(), img_path, nrow=1)\n",
    "            frames.append(imageio.imread(img_path))\n",
    "\n",
    "    imageio.mimsave(save_path, frames, fps=10)\n",
    "    print(f\"Анимация латентного перехода сохранена в {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf1af16",
   "metadata": {},
   "source": [
    "### Примеры использования"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3184ea2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Меняем 0-й и 1-й признак\n",
    "z = generate_manual_z(fixed_value=0.0, vary_dims=(0, 1), range_min=-3, range_max=3)\n",
    "generate_face_grid(z, save_path=f'{GRID_FOLDER}/grid_faces_0_1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a401ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Меняем 2-й и 3-й признак\n",
    "z = generate_manual_z(fixed_value=0.0, vary_dims=(2, 3), range_min=-2, range_max=2)\n",
    "generate_face_grid(z, save_path=f'{GRID_FOLDER}/grid_faces_2_3.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be08345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Узкий диапазон изменений для тонких вариаций\n",
    "z = generate_manual_z(fixed_value=0.0, vary_dims=(5, 6), range_min=-1, range_max=1)\n",
    "generate_face_grid(z, save_path=f'{GRID_FOLDER}/grid_faces_5_6.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9671198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Генерация анимации перемещения по латентному пространству\n",
    "generate_latent_animation(vary_dims=(0, 1), steps=40, save_path=f'{OUTPUT}/latent_walk.gif')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
