{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2b95e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Импорты ---\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "import requests\n",
    "\n",
    "import torchvision\n",
    "import torchvision.utils as vutils\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc025b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Гиперпараметры обучения и модели ---\n",
    "BATCH_SIZE = 128\n",
    "# Размер пакета данных для одной итерации обучения.\n",
    "# Большее значение ускоряет обучение, но требует больше видеопамяти.\n",
    "LATENT_DIM = 128\n",
    "# Размерность латентного пространства (вектора z).\n",
    "# Чем больше размерность, тем богаче потенциальная вариативность генерируемых изображений.\n",
    "IMAGE_SIZE = 128\n",
    "# Размер входных изображений (высота и ширина в пикселях).\n",
    "# Должен быть кратным 16 для корректной работы архитектуры энкодера и декодера.\n",
    "EPOCHS = 30\n",
    "# Общее количество проходов по всему датасету во время обучения.\n",
    "LEARNING_RATE = 1e-3\n",
    "# Скорость обучения оптимизатора.\n",
    "# Малое значение приводит к медленному обучению, большое может вызывать нестабильность.\n",
    "SAVE_PATH = './models/vae_model.pth'\n",
    "# Путь для сохранения финальной версии обученной модели.\n",
    "BEST_MODEL_PATH = './models/vae_best_model.pth'\n",
    "# Путь для сохранения лучшей модели (по минимальному значению лосса на обучении).\n",
    "SAVE_INTERVAL = 50\n",
    "# Интервал в эпохах для сохранения сгенерированных изображений в процессе обучения.\n",
    "OLD_MODEL = False\n",
    "# Путь к ранее обученной модели для продолжения обучения или генерации изображений.\n",
    "# False означает обучение \\\"с нуля\\\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be2ccbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Folders\n",
    "OUTPUT = './output'\n",
    "EPOCHS_FOLDER = os.path.join(OUTPUT, 'epochs')\n",
    "GENERATED_FOLDER = os.path.join(OUTPUT, 'generated')\n",
    "MORPH_FOLDER = os.path.join(OUTPUT, 'morph_frames')\n",
    "Z_FOLDER = os.path.join(OUTPUT, 'z_frames')\n",
    "LAT_GRID = os.path.join(OUTPUT, 'latent_frames')\n",
    "GRID_FOLDER = os.path.join(OUTPUT, 'grid')\n",
    "os.makedirs(OUTPUT, exist_ok=True)\n",
    "os.makedirs(os.path.dirname(EPOCHS_FOLDER), exist_ok=True)\n",
    "os.makedirs(os.path.dirname(GENERATED_FOLDER), exist_ok=True)\n",
    "os.makedirs(os.path.dirname(MORPH_FOLDER), exist_ok=True)\n",
    "os.makedirs(os.path.dirname(Z_FOLDER), exist_ok=True)\n",
    "os.makedirs(os.path.dirname(LAT_GRID), exist_ok=True)\n",
    "os.makedirs(os.path.dirname(GRID_FOLDER), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a98e16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.DEVICE('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using DEVICE: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13edddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подготовка данных\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e496c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.ImageFolder(root='data', transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "print(f\"Loaded {len(dataset)} images from 'data'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97849f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определение модели\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim, img_size):\n",
    "        super().__init__()\n",
    "        assert img_size % 16 == 0, \"IMAGE_SIZE должно быть кратно 16\"\n",
    "        num_layers = int(torch.log2(torch.tensor(img_size // 4)))\n",
    "        layers = []\n",
    "        in_channels = 3\n",
    "        for _ in range(num_layers):\n",
    "            out_channels = in_channels * 2\n",
    "            layers.append(nn.Conv2d(in_channels, out_channels, 4, 2, 1))\n",
    "            layers.append(nn.ReLU())\n",
    "            in_channels = out_channels\n",
    "        self.conv = nn.Sequential(*layers)\n",
    "        self.fc_mu = nn.Linear(in_channels * 4 * 4, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(in_channels * 4 * 4, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        return mu, logvar\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, img_size):\n",
    "        super().__init__()\n",
    "        assert img_size % 16 == 0, \"IMAGE_SIZE должно быть кратно 16\"\n",
    "        num_layers = int(torch.log2(torch.tensor(img_size // 4)))\n",
    "        self.start_channels = 3 * (2 ** num_layers)\n",
    "        self.fc = nn.Linear(latent_dim, self.start_channels * 4 * 4)\n",
    "        layers = []\n",
    "        in_channels = self.start_channels\n",
    "        for _ in range(num_layers):\n",
    "            out_channels = in_channels // 2\n",
    "            layers.append(nn.ConvTranspose2d(in_channels, out_channels, 4, 2, 1))\n",
    "            layers.append(nn.ReLU())\n",
    "            in_channels = out_channels\n",
    "        layers[-2] = nn.ConvTranspose2d(in_channels * 2, 3, 4, 2, 1)\n",
    "        layers[-1] = nn.Sigmoid()\n",
    "        self.deconv = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = self.fc(z)\n",
    "        x = x.view(-1, self.start_channels, 4, 4)\n",
    "        x = self.deconv(x)\n",
    "        return x\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_dim, img_size):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(latent_dim, img_size)\n",
    "        self.decoder = Decoder(latent_dim, img_size)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encoder(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        recon_x = self.decoder(z)\n",
    "        return recon_x, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19a8f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вспомогательные функции\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    recon_loss = nn.functional.mse_loss(recon_x, x, reduction='sum')\n",
    "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return recon_loss + kl_loss\n",
    "\n",
    "def plot_loss(losses):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(losses, label='Training Loss')\n",
    "    plt.xlabel('Эпоха')\n",
    "    plt.ylabel('Потери')\n",
    "    plt.title('График изменения потерь')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def imshow(img):\n",
    "    img = np.array(img)\n",
    "    if img.min() < 0:\n",
    "        img -= img.min()\n",
    "        img /= img.max()\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(np.transpose(img, (1, 2, 0)))\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb4f93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализация модели\n",
    "vae = VAE(LATENT_DIM, IMAGE_SIZE).to(DEVICE)\n",
    "if OLD_MODEL:\n",
    "    vae.load_state_dict(torch.load(OLD_MODEL, map_location=DEVICE))\n",
    "    print(\"Loaded model from\", OLD_MODEL)\n",
    "optimizer = optim.Adam(vae.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a952c703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучение модели\n",
    "losses = []\n",
    "best_loss = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26205e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    vae.train()\n",
    "    train_loss = 0\n",
    "    for imgs, _ in dataloader:\n",
    "        imgs = imgs.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        recon_imgs, mu, logvar = vae(imgs)\n",
    "        loss = loss_function(recon_imgs, imgs, mu, logvar)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    avg_loss = train_loss / len(dataset)\n",
    "    losses.append(avg_loss)\n",
    "    print(f'Epoch [{epoch+1}/{EPOCHS}] - Loss: {avg_loss:.4f}')\n",
    "\n",
    "    # Сохраняем лучшую модель\n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        torch.save(vae.state_dict(), BEST_MODEL_PATH)\n",
    "        print(f\"New best model saved at epoch {epoch+1} with loss {best_loss:.4f}\")\n",
    " \n",
    "    # Периодически генерируем изображения\n",
    "    if (epoch + 1) % SAVE_INTERVAL == 0:\n",
    "        vae.eval()\n",
    "        with torch.no_grad():\n",
    "            z = torch.randn(16, LATENT_DIM).to(DEVICE)\n",
    "            samples = vae.decoder(z)\n",
    "        vutils.save_image(samples.cpu(), f'{EPOCHS_FOLDER}/generated_faces_epoch_{epoch+1}.png', nrow=4)\n",
    "        imshow(torchvision.utils.make_grid(samples.cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4982ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохранение финальной модели\n",
    "torch.save(vae.state_dict(), SAVE_PATH)\n",
    "print(f\"Final model saved to {SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30de4091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Генерация итоговых лиц\n",
    "vae.eval()\n",
    "with torch.no_grad():\n",
    "    z = torch.randn(16, LATENT_DIM).to(DEVICE)\n",
    "    samples = vae.decoder(z)\n",
    "vutils.save_image(samples.cpu(), f'{GENERATED_FOLDER}/generated_faces_final.png', nrow=4)\n",
    "imshow(torchvision.utils.make_grid(samples.cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb82ced1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Построение графика потерь\n",
    "plot_loss(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46831f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Уведомление о завершении обучения\n",
    "url = \"http://home.teyhd.ru:3334/\"\n",
    "params = {\"msg\": f\"Обучение завершено: Эпох {EPOCHS}\\nФинальный лосс: {losses[-1]:.6f}\"}\n",
    "try:\n",
    "    response = requests.get(url, params=params)\n",
    "    print(f\"Notification sent! Status Code: {response.status_code} | Response Text: {response.text}\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to send notification: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12a4162",
   "metadata": {},
   "source": [
    "### Генерация разных типов z для декодера и создание GIF морфинга"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39aa048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Настройки ---\n",
    "NUM_PIC = 16  # Количество изображений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e35ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Стандартное сэмплирование (N(0,1)) ---\n",
    "with torch.no_grad():\n",
    "    z = torch.randn(NUM_PIC, LATENT_DIM).to(DEVICE)\n",
    "    samples = vae.decoder(z)\n",
    "    vutils.save_image(samples.cpu(), f'{Z_FOLDER}/z_random_normal.png', nrow=4)\n",
    "    imshow(torchvision.utils.make_grid(samples.cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b879f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Ручное задание признаков ---\n",
    "with torch.no_grad():\n",
    "    z = torch.zeros(NUM_PIC, LATENT_DIM).to(DEVICE)\n",
    "    z[:, :2] = torch.linspace(-3, 3, NUM_PIC).unsqueeze(1).repeat(1, 2)\n",
    "    samples = vae.decoder(z)\n",
    "    vutils.save_image(samples.cpu(), f'{Z_FOLDER}/z_manual_features.png', nrow=4)\n",
    "    imshow(torchvision.utils.make_grid(samples.cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7489685a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Латентные коды реальных изображений ---\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe195ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_dataset = datasets.ImageFolder(root='data', transform=transform)\n",
    "real_loader = DataLoader(real_dataset, batch_size=NUM_PIC, shuffle=True)\n",
    "\n",
    "real_imgs, _ = next(iter(real_loader))\n",
    "real_imgs = real_imgs.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d56475",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    mu, logvar = vae.encoder(real_imgs)\n",
    "    z = vae.reparameterize(mu, logvar)\n",
    "    samples = vae.decoder(z)\n",
    "    vutils.save_image(samples.cpu(), f'{Z_FOLDER}/z_from_real_imgs.png', nrow=4)\n",
    "    imshow(torchvision.utils.make_grid(samples.cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a38f870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Интерполяция между двумя точками ---\n",
    "frames = []\n",
    "with torch.no_grad():\n",
    "    z1 = torch.randn(1, LATENT_DIM).to(DEVICE)\n",
    "    z2 = torch.randn(1, LATENT_DIM).to(DEVICE)\n",
    "    alphas = torch.linspace(0, 1, steps=30).unsqueeze(1).to(DEVICE)\n",
    "    z = (1 - alphas) * z1 + alphas * z2\n",
    "    for i, zi in enumerate(z):\n",
    "        sample = vae.decoder(zi.unsqueeze(0))\n",
    "        img_path = f'{MORPH_FOLDER}/frame_{i:03d}.png'\n",
    "        vutils.save_image(sample.cpu(), img_path, nrow=1)\n",
    "        frames.append(imageio.imread(img_path))\n",
    "    imageio.mimsave(f'{MORPH_FOLDER}/morphing.gif', frames, fps=10)\n",
    "    print(\"GIF морфинга сохранён как 'morphing.gif'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd05ebf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Показываем последний батч интерполяции\n",
    "vutils.save_image(vae.decoder(z).cpu(), f'{Z_FOLDER}z_interpolation.png', nrow=6)\n",
    "imshow(torchvision.utils.make_grid(vae.decoder(z).cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5b0292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Альтернативное распределение (равномерное) ---\n",
    "with torch.no_grad():\n",
    "    z = torch.empty(NUM_PIC, LATENT_DIM).uniform_(-2, 2).to(DEVICE)\n",
    "    samples = vae.decoder(z)\n",
    "    vutils.save_image(samples.cpu(), f'{Z_FOLDER}z_uniform_distribution.png', nrow=4)\n",
    "    imshow(torchvision.utils.make_grid(samples.cpu()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2625482a",
   "metadata": {},
   "source": [
    "### Код для удобного подбора ручных признаков и генерации 2D-сетки лиц + анимация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24b4bad0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (encoder): Encoder(\n",
       "    (conv): Sequential(\n",
       "      (0): Conv2d(3, 6, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (1): ReLU()\n",
       "      (2): Conv2d(6, 12, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (3): ReLU()\n",
       "      (4): Conv2d(12, 24, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (5): ReLU()\n",
       "      (6): Conv2d(24, 48, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (7): ReLU()\n",
       "      (8): Conv2d(48, 96, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (9): ReLU()\n",
       "    )\n",
       "    (fc_mu): Linear(in_features=1536, out_features=128, bias=True)\n",
       "    (fc_logvar): Linear(in_features=1536, out_features=128, bias=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (fc): Linear(in_features=128, out_features=1536, bias=True)\n",
       "    (deconv): Sequential(\n",
       "      (0): ConvTranspose2d(96, 48, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (1): ReLU()\n",
       "      (2): ConvTranspose2d(48, 24, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (3): ReLU()\n",
       "      (4): ConvTranspose2d(24, 12, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (5): ReLU()\n",
       "      (6): ConvTranspose2d(12, 6, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (7): ReLU()\n",
       "      (8): ConvTranspose2d(6, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (9): Sigmoid()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Настройки ---\n",
    "GRID_SIZE = 8  # Сетка GRID_SIZE x GRID_SIZE\n",
    "# Предполагаем, что модель VAE уже загружена и называется \"vae\"\n",
    "vae.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "262e04bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Удобный подбор параметров ---\n",
    "def generate_manual_z(fixed_value=0.0, vary_dims=(0, 1), range_min=-3, range_max=3, grid_size=GRID_SIZE):\n",
    "    total = grid_size * grid_size\n",
    "    z = torch.full((total, LATENT_DIM), fixed_value).to(DEVICE)\n",
    "    values = torch.linspace(range_min, range_max, steps=grid_size)\n",
    "    grid = torch.cartesian_prod(values, values)\n",
    "    for idx, (val_x, val_y) in enumerate(grid):\n",
    "        z[idx, vary_dims[0]] = val_x\n",
    "        z[idx, vary_dims[1]] = val_y\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7a498b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GRID_FOLDER' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# --- Генерация 2D-сетки лиц ---\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_face_grid\u001b[39m(z, save_path=\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mGRID_FOLDER\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/grid_faces.png\u001b[39m\u001b[33m'\u001b[39m, grid_size=GRID_SIZE):\n\u001b[32m      3\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m      4\u001b[39m         samples = vae.decoder(z)\n",
      "\u001b[31mNameError\u001b[39m: name 'GRID_FOLDER' is not defined"
     ]
    }
   ],
   "source": [
    "# --- Генерация 2D-сетки лиц ---\n",
    "def generate_face_grid(z, save_path=f'{GRID_FOLDER}/grid_faces.png', grid_size=GRID_SIZE):\n",
    "    with torch.no_grad():\n",
    "        samples = vae.decoder(z)\n",
    "    vutils.save_image(samples.cpu(), save_path, nrow=grid_size)\n",
    "    print(f\"Сетка лиц сохранена в {save_path}\")\n",
    "    imshow(torchvision.utils.make_grid(samples.cpu(), nrow=grid_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a3ed4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Анимация перемещения по латентному пространству ---\n",
    "def generate_latent_animation(vary_dims=(0, 1), steps=30, fixed_value=0.0, range_min=-3, range_max=3, save_path='latent_walk.gif'):\n",
    "    frames = []\n",
    "  \n",
    "    with torch.no_grad():\n",
    "        for step in range(steps):\n",
    "            alpha = step / (steps - 1)\n",
    "            val_x = (1 - alpha) * range_min + alpha * range_max\n",
    "            val_y = np.sin(alpha * np.pi * 2) * (range_max / 2)\n",
    "\n",
    "            z = torch.full((1, LATENT_DIM), fixed_value).to(DEVICE)\n",
    "            z[0, vary_dims[0]] = val_x\n",
    "            z[0, vary_dims[1]] = val_y\n",
    "\n",
    "            sample = vae.decoder(z)\n",
    "            img_path = f'{LAT_GRID}/frame_{step:03d}.png'\n",
    "            vutils.save_image(sample.cpu(), img_path, nrow=1)\n",
    "            frames.append(imageio.imread(img_path))\n",
    "\n",
    "    imageio.mimsave(save_path, frames, fps=10)\n",
    "    print(f\"Анимация латентного перехода сохранена в {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3184ea2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Примеры использования ---\n",
    "\n",
    "# 1. Меняем 0-й и 1-й признак\n",
    "z = generate_manual_z(fixed_value=0.0, vary_dims=(0, 1), range_min=-3, range_max=3)\n",
    "generate_face_grid(z, save_path=f'{GRID_FOLDER}grid_faces_0_1.png')\n",
    "\n",
    "# 2. Меняем 2-й и 3-й признак\n",
    "z = generate_manual_z(fixed_value=0.0, vary_dims=(2, 3), range_min=-2, range_max=2)\n",
    "generate_face_grid(z, save_path=f'{GRID_FOLDER}grid_faces_2_3.png')\n",
    "\n",
    "# 3. Узкий диапазон изменений для тонких вариаций\n",
    "z = generate_manual_z(fixed_value=0.0, vary_dims=(5, 6), range_min=-1, range_max=1)\n",
    "generate_face_grid(z, save_path=f'{GRID_FOLDER}grid_faces_5_6.png')\n",
    "\n",
    "# 4. Генерация анимации перемещения по латентному пространству\n",
    "generate_latent_animation(vary_dims=(0, 1), steps=40, save_path=f'{OUTPUT}latent_walk.gif')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
