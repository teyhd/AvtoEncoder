{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b2b95e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Импорты ---\n",
    "import os\n",
    "import datetime\n",
    "import multiprocessing\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio.v2 as imageio\n",
    "import requests\n",
    "\n",
    "import torchvision\n",
    "import torchvision.utils as vutils\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc025b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Гиперпараметры обучения и модели ---\n",
    "VAE_W = 2 # Версия VAE\n",
    "SIMPLE_LOSS = False\n",
    "\n",
    "BATCH_SIZE = 128 #4 # Размер пакета данных для одной итерации обучения.Большее значение ускоряет обучение, но требует больше видеопамяти.\n",
    "LATENT_DIM = 128 #2 # Размерность латентного пространства (вектора z). # Чем больше размерность, тем богаче потенциальная вариативность генерируемых изображений.\n",
    "IMAGE_SIZE = 256 #2 # Размер входных изображений (высота и ширина в пикселях).# Должен быть кратным 16 для корректной работы архитектуры энкодера и декодера.\n",
    "\n",
    "EPOCHS = 300\n",
    "EARLY_STOP = 20 #30 # Общее количество проходов по всему датасету во время обучения.\n",
    "\n",
    "SAVE_INTERVAL = 5 # Интервал в эпохах для сохранения сгенерированных изображений в процессе обучения.\n",
    "\n",
    "OLD_MODEL = './models/vae_model_V3.pth' #False\n",
    "SAVE_PATH = './models/vae_model_V4.pth'# Путь для сохранения финальной версии обученной модели.\n",
    "\n",
    "LEARNING_RATE = 1e-3 # Скорость обучения оптимизатора.  # Малое значение приводит к медленному обучению, большое может вызывать нестабильность."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8be2ccbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Folders\n",
    "OUTPUT = './output'\n",
    "EPOCHS_FOLDER = os.path.join(OUTPUT, 'epochs')\n",
    "EPOCHS_FOLDER_REAL = os.path.join(OUTPUT, 'epochs_real')\n",
    "GENERATED_FOLDER = os.path.join(OUTPUT, 'generated')\n",
    "MORPH_FOLDER = os.path.join(OUTPUT, 'morph_frames')\n",
    "Z_FOLDER = os.path.join(OUTPUT, 'z_frames')\n",
    "LAT_GRID = os.path.join(OUTPUT, 'latent_frames')\n",
    "GRID_FOLDER = os.path.join(OUTPUT, 'grid')\n",
    "os.makedirs(OUTPUT, exist_ok=True)\n",
    "os.makedirs(EPOCHS_FOLDER, exist_ok=True)\n",
    "os.makedirs(EPOCHS_FOLDER_REAL, exist_ok=True)\n",
    "os.makedirs(GENERATED_FOLDER, exist_ok=True)\n",
    "os.makedirs(MORPH_FOLDER, exist_ok=True)\n",
    "os.makedirs(Z_FOLDER, exist_ok=True)\n",
    "os.makedirs(LAT_GRID, exist_ok=True)\n",
    "os.makedirs(GRID_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fabaf893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание загрузчиков данных\n",
    "num_workers = min(16, multiprocessing.cpu_count())\n",
    "prefetch_factor = 128 * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a98e16d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using DEVICE: cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using DEVICE: {DEVICE}\")\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e13edddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подготовка данных\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e496c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загрузил: 1800 Изображеннй\n"
     ]
    }
   ],
   "source": [
    "dataset = datasets.ImageFolder(root='data', transform=transform)\n",
    "#dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,           # обязательно перемешивание\n",
    "    num_workers=num_workers,           # или другое оптимальное число\n",
    "    pin_memory=True,\n",
    "    prefetch_factor=prefetch_factor,       # если много CPU\n",
    "    persistent_workers=True  # если поддерживается версией\n",
    ")\n",
    "\n",
    "print(f\"Загрузил: {len(dataset)} Изображеннй\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d19a8f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    recon_loss = nn.functional.mse_loss(recon_x, x, reduction='sum')\n",
    "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return recon_loss + kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d065a6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вспомогательные функции\n",
    "def plot_loss(losses):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(losses, label='Training Loss')\n",
    "    plt.xlabel('Эпоха')\n",
    "    plt.ylabel('Потери')\n",
    "    plt.title('График изменения потерь')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def imshow(img):\n",
    "    img = np.array(img)\n",
    "    if img.min() < 0:\n",
    "        img -= img.min()\n",
    "        img /= img.max()\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(np.transpose(img, (1, 2, 0)))\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f84d5116",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ВАЛИДАЦИЯ ПО ЭПОХАМ НА РЕАЛЬНЫХ ФОТО\n",
    "NUM_PIC = 32\n",
    "real_dataset = datasets.ImageFolder(root='data', transform=transform)\n",
    "real_loader = DataLoader(real_dataset, batch_size=NUM_PIC, shuffle=True)\n",
    "\n",
    "real_imgs, _ = next(iter(real_loader))\n",
    "real_imgs = real_imgs.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3cb4f93d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from: ./models/vae_model_V3.pth | OLD_EPOCH: 357\n",
      "VAE_V: 2 | IMAGE_SIZE: 256 | LATENT_DIM: 128 | BATCH_SIZE: 128\n"
     ]
    }
   ],
   "source": [
    "# Инициализация модели\n",
    "if OLD_MODEL:\n",
    "    checkpoint = torch.load(OLD_MODEL, map_location=DEVICE)\n",
    "    VAE_W = checkpoint['v']\n",
    "    module = importlib.import_module(f'VAE.vae_v{VAE_W}')\n",
    "    VAE = module.VAE\n",
    "    LATENT_DIM = checkpoint['LATENT_DIM']\n",
    "    IMAGE_SIZE = checkpoint['IMAGE_SIZE']\n",
    "    BATCH_SIZE = checkpoint['BATCH_SIZE']\n",
    "    vae = VAE(LATENT_DIM, IMAGE_SIZE).to(DEVICE)\n",
    "    vae.load_state_dict(checkpoint['model'])\n",
    "    OLD_EPOCH = checkpoint['EPOCH']\n",
    "    print(f\"Loaded model from: {OLD_MODEL} | OLD_EPOCH: {OLD_EPOCH}\")\n",
    "    print(f'VAE_V: {VAE_W} | IMAGE_SIZE: {IMAGE_SIZE} | LATENT_DIM: {LATENT_DIM} | BATCH_SIZE: {BATCH_SIZE}')\n",
    "else:\n",
    "    OLD_EPOCH = 0\n",
    "    module = importlib.import_module(f'VAE.vae_v{VAE_W}')\n",
    "    VAE = module.VAE\n",
    "    vae = VAE(LATENT_DIM, IMAGE_SIZE).to(DEVICE)\n",
    "perceptual_loss_fn = module.VGGPerceptualLoss(device=DEVICE).to(DEVICE)\n",
    "optimizer = optim.Adam(vae.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a952c703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучение модели\n",
    "losses = []\n",
    "best_loss = float('inf')\n",
    "best_model = vae\n",
    "epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26205e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    tstart = datetime.datetime.now()\n",
    "    vae.train()\n",
    "    train_loss = 0\n",
    "    recon_loss_total = 0\n",
    "    kl_loss_total = 0\n",
    "\n",
    "    for imgs, _ in dataloader:\n",
    "        imgs = imgs.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        recon_imgs, mu, logvar = vae(imgs)\n",
    "        \n",
    "        if SIMPLE_LOSS:\n",
    "            loss = loss_function(recon_imgs, imgs, mu, logvar)\n",
    "        else:\n",
    "            # Перцептуальная + KL-дивергенция\n",
    "            perceptual = perceptual_loss_fn(recon_imgs, imgs)\n",
    "            kl = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "            loss = perceptual + 0.05 * kl  # веса можно регулировать\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        if SIMPLE_LOSS == False:\n",
    "            recon_loss_total += perceptual.item()\n",
    "            kl_loss_total += kl.item()\n",
    "\n",
    "    avg_loss = train_loss / len(dataloader)\n",
    "    if SIMPLE_LOSS == False:\n",
    "        avg_recon_loss = recon_loss_total / len(dataloader)\n",
    "        avg_kl_loss = kl_loss_total / len(dataloader)\n",
    "    else:\n",
    "        avg_recon_loss = 0\n",
    "        avg_kl_loss = 0\n",
    "\n",
    "    losses.append(avg_loss)\n",
    "\n",
    "    print(f\"\\nEpoch [{epoch + 1}/{EPOCHS}]\")\n",
    "    print(f\"Train Loss: {avg_loss:.4f} (Recon: {avg_recon_loss:.4f}, KL: {avg_kl_loss:.4f})\")\n",
    "    print(f\"Время: {str(datetime.datetime.now() - tstart).split('.')[0]}\")\n",
    "\n",
    "    # Сохраняем лучшую модель\n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        best_model = vae\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"val_loss: {avg_loss:.4f} > best_loss: {best_loss:.4f}, попытка {patience_counter}\")\n",
    "        if patience_counter >= EARLY_STOP:\n",
    "            print(\"Ранняя остановка!\")\n",
    "            break\n",
    "\n",
    "    # Периодическая генерация\n",
    "    if (epoch + 1) % SAVE_INTERVAL == 0:\n",
    "        vae.eval()\n",
    "        if 1>5:\n",
    "            with torch.no_grad():\n",
    "                z = torch.randn(4, LATENT_DIM).to(DEVICE)\n",
    "                samples = vae.decoder(z)\n",
    "            vutils.save_image(samples.cpu(), f'{EPOCHS_FOLDER}/generated_faces_epoch_{epoch+1}.png', nrow=4)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            mu, logvar = vae.encoder(imgs)\n",
    "            z = vae.reparameterize(mu, logvar)\n",
    "            samples = vae.decoder(z)\n",
    "            vutils.save_image(samples.cpu(), f'{EPOCHS_FOLDER_REAL}/z_from_real_imgs_{OLD_EPOCH + epoch + 1}.png', nrow=4)\n",
    "            imshow(torchvision.utils.make_grid(samples.cpu()))\n",
    "    if best_loss<1:\n",
    "        print(\"Ранняя остановка!\")\n",
    "        break\n",
    "\n",
    "    print(f\"Все время: {str(datetime.datetime.now() - tstart).split('.')[0]}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac90b33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BESTLoss = 250\n",
    "best_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4982ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохранение финальной модели\n",
    "if best_loss <= BESTLoss:\n",
    "    torch.save({\n",
    "        \"model\": best_model.state_dict(),\n",
    "        \"BATCH_SIZE\": BATCH_SIZE,\n",
    "        \"LATENT_DIM\":LATENT_DIM,\n",
    "        \"IMAGE_SIZE\":IMAGE_SIZE,\n",
    "        \"EPOCH\": OLD_EPOCH + epoch + 1,\n",
    "        \"v\": VAE_W\n",
    "    }, SAVE_PATH)\n",
    "\n",
    "    print(f\"Final model saved to {SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d868f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Латентные коды реальных изображений ---\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fddef11",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_PIC = 32\n",
    "real_dataset = datasets.ImageFolder(root='data', transform=transform)\n",
    "real_loader = DataLoader(real_dataset, batch_size=NUM_PIC, shuffle=True)\n",
    "\n",
    "real_imgs, _ = next(iter(real_loader))\n",
    "real_imgs = real_imgs.to(DEVICE)\n",
    "\n",
    "with torch.no_grad():\n",
    "    mu, logvar = best_model.encoder(real_imgs)\n",
    "    z = best_model.reparameterize(mu, logvar)\n",
    "    samples = best_model.decoder(z)\n",
    "    vutils.save_image(samples.cpu(), f'{Z_FOLDER}/z_from_real_imgs.png', nrow=4)\n",
    "    imshow(torchvision.utils.make_grid(samples.cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30de4091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Генерация итоговых лиц\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    z = torch.randn(16, LATENT_DIM).to(DEVICE)\n",
    "    samples = best_model.decoder(z)\n",
    "vutils.save_image(samples.cpu(), f'{GENERATED_FOLDER}/generated_faces_final.png', nrow=4)\n",
    "imshow(torchvision.utils.make_grid(samples.cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb82ced1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Построение графика потерь\n",
    "plot_loss(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46831f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Уведомление о завершении обучения\n",
    "url = \"http://home.teyhd.ru:3334/\"\n",
    "params = {\"msg\": f\"Обучение Автоэнкодера завершено: Эпох {EPOCHS}\\nФинальный лосс: {losses[-1]:.6f}\"}\n",
    "try:\n",
    "    response = requests.get(url, params=params)\n",
    "    print(f\"Notification sent! Status Code: {response.status_code} | Response Text: {response.text}\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to send notification: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12a4162",
   "metadata": {},
   "source": [
    "### Генерация разных типов z для декодера и создание GIF морфинга"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39aa048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Настройки ---\n",
    "NUM_PIC = 16  # Количество изображений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e35ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Стандартное сэмплирование (N(0,1)) ---\n",
    "with torch.no_grad():\n",
    "    z = torch.randn(NUM_PIC, LATENT_DIM).to(DEVICE)\n",
    "    samples = vae.decoder(z)\n",
    "    vutils.save_image(samples.cpu(), f'{Z_FOLDER}/z_random_normal.png', nrow=4)\n",
    "    imshow(torchvision.utils.make_grid(samples.cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b879f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Ручное задание признаков ---\n",
    "NUM_PIC = 16\n",
    "with torch.no_grad():\n",
    "    z = torch.zeros(NUM_PIC, LATENT_DIM).to(DEVICE)\n",
    "    z[:, :2] = torch.linspace(-3, 3, NUM_PIC).unsqueeze(1).repeat(1, 2)\n",
    "    samples = vae.decoder(z)\n",
    "    vutils.save_image(samples.cpu(), f'{Z_FOLDER}/z_manual_features.png', nrow=4)\n",
    "    imshow(torchvision.utils.make_grid(samples.cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a38f870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Интерполяция между двумя точками ---\n",
    "frames = []\n",
    "with torch.no_grad():\n",
    "    z1 = torch.randn(1, LATENT_DIM).to(DEVICE)\n",
    "    z2 = torch.randn(1, LATENT_DIM).to(DEVICE)\n",
    "    alphas = torch.linspace(0, 1, steps=30).unsqueeze(1).to(DEVICE)\n",
    "    z = (1 - alphas) * z1 + alphas * z2\n",
    "    for i, zi in enumerate(z):\n",
    "        sample = vae.decoder(zi.unsqueeze(0))\n",
    "        img_path = f'{MORPH_FOLDER}/frame_{i:03d}.png'\n",
    "        vutils.save_image(sample.cpu(), img_path, nrow=1)\n",
    "        frames.append(imageio.imread(img_path))\n",
    "    imageio.mimsave(f'{MORPH_FOLDER}/morphing.gif', frames, fps=10)\n",
    "    print(\"GIF морфинга сохранён как 'morphing.gif'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd05ebf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Показываем последний батч интерполяции\n",
    "vutils.save_image(vae.decoder(z).cpu(), f'{Z_FOLDER}/z_interpolation.png', nrow=6)\n",
    "imshow(torchvision.utils.make_grid(vae.decoder(z).cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5b0292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Альтернативное распределение (равномерное) ---\n",
    "with torch.no_grad():\n",
    "    z = torch.empty(NUM_PIC, LATENT_DIM).uniform_(0, 16).to(DEVICE)\n",
    "    samples = vae.decoder(z)\n",
    "    vutils.save_image(samples.cpu(), f'{Z_FOLDER}/z_uniform_distribution.png', nrow=4)\n",
    "    imshow(torchvision.utils.make_grid(samples.cpu()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2625482a",
   "metadata": {},
   "source": [
    "### Код для удобного подбора ручных признаков и генерации 2D-сетки лиц + анимация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b4bad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Настройки ---\n",
    "GRID_SIZE = 8  # Сетка GRID_SIZE x GRID_SIZE\n",
    "# Предполагаем, что модель VAE уже загружена и называется \"vae\"\n",
    "vae.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262e04bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Удобный подбор параметров ---\n",
    "def generate_manual_z(fixed_value=0.0, vary_dims=(0, 1), range_min=-3, range_max=3, grid_size=GRID_SIZE):\n",
    "    total = grid_size * grid_size\n",
    "    z = torch.full((total, LATENT_DIM), fixed_value).to(DEVICE)\n",
    "    values = torch.linspace(range_min, range_max, steps=grid_size)\n",
    "    grid = torch.cartesian_prod(values, values)\n",
    "    for idx, (val_x, val_y) in enumerate(grid):\n",
    "        z[idx, vary_dims[0]] = val_x\n",
    "        z[idx, vary_dims[1]] = val_y\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a498b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Генерация 2D-сетки лиц ---\n",
    "def generate_face_grid(z, save_path=f'{GRID_FOLDER}/grid_faces.png', grid_size=GRID_SIZE):\n",
    "    with torch.no_grad():\n",
    "        samples = vae.decoder(z)\n",
    "    vutils.save_image(samples.cpu(), save_path, nrow=grid_size)\n",
    "    print(f\"Сетка лиц сохранена в {save_path}\")\n",
    "    imshow(torchvision.utils.make_grid(samples.cpu(), nrow=grid_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a3ed4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Анимация перемещения по латентному пространству ---\n",
    "def generate_latent_animation(vary_dims=(0, 1), steps=30, fixed_value=0.0, range_min=-3, range_max=3, save_path='{OUTPUT}/latent_walk.gif'):\n",
    "    frames = []\n",
    "  \n",
    "    with torch.no_grad():\n",
    "        for step in range(steps):\n",
    "            alpha = step / (steps - 1)\n",
    "            val_x = (1 - alpha) * range_min + alpha * range_max\n",
    "            val_y = np.sin(alpha * np.pi * 2) * (range_max / 2)\n",
    "\n",
    "            z = torch.full((1, LATENT_DIM), fixed_value).to(DEVICE)\n",
    "            z[0, vary_dims[0]] = val_x\n",
    "            z[0, vary_dims[1]] = val_y\n",
    "\n",
    "            sample = vae.decoder(z)\n",
    "            img_path = f'{LAT_GRID}/frame_{step:03d}.png'\n",
    "            vutils.save_image(sample.cpu(), img_path, nrow=1)\n",
    "            frames.append(imageio.imread(img_path))\n",
    "\n",
    "    imageio.mimsave(save_path, frames, fps=10)\n",
    "    print(f\"Анимация латентного перехода сохранена в {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf1af16",
   "metadata": {},
   "source": [
    "### Примеры использования"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3184ea2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Меняем 0-й и 1-й признак\n",
    "z = generate_manual_z(fixed_value=0.0, vary_dims=(0, 1), range_min=-3, range_max=3)\n",
    "generate_face_grid(z, save_path=f'{GRID_FOLDER}/grid_faces_0_1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a401ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Меняем 2-й и 3-й признак\n",
    "z = generate_manual_z(fixed_value=0.0, vary_dims=(3, 8), range_min=16, range_max=32*2)\n",
    "generate_face_grid(z, save_path=f'{GRID_FOLDER}/grid_faces_2_3.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be08345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Узкий диапазон изменений для тонких вариаций\n",
    "z = generate_manual_z(fixed_value=0.0, vary_dims=(5, 6), range_min=-1, range_max=1)\n",
    "generate_face_grid(z, save_path=f'{GRID_FOLDER}/grid_faces_5_6.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9671198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Генерация анимации перемещения по латентному пространству\n",
    "generate_latent_animation(vary_dims=(0, 127), steps=128, save_path=f'{OUTPUT}/latent_walk.gif')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
