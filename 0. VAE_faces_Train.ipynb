{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b2b95e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Импорты ---\n",
    "import os\n",
    "import datetime\n",
    "import multiprocessing\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio.v2 as imageio\n",
    "import requests\n",
    "\n",
    "import torchvision\n",
    "import torchvision.utils as vutils\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from VAE.vae_v2 import VAE, VGGPerceptualLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc025b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Гиперпараметры обучения и модели ---\n",
    "BATCH_SIZE = 128 #4\n",
    "# Размер пакета данных для одной итерации обучения.\n",
    "# Большее значение ускоряет обучение, но требует больше видеопамяти.\n",
    "LATENT_DIM = 1024 #2\n",
    "# Размерность латентного пространства (вектора z).\n",
    "# Чем больше размерность, тем богаче потенциальная вариативность генерируемых изображений.\n",
    "IMAGE_SIZE = 256 #2\n",
    "# Размер входных изображений (высота и ширина в пикселях).\n",
    "# Должен быть кратным 16 для корректной работы архитектуры энкодера и декодера.\n",
    "EPOCHS = 500\n",
    "EARLY_STOP = 20 #30\n",
    "# Общее количество проходов по всему датасету во время обучения.\n",
    "LEARNING_RATE = 1e-3\n",
    "# Скорость обучения оптимизатора.\n",
    "# Малое значение приводит к медленному обучению, большое может вызывать нестабильность.\n",
    "SAVE_INTERVAL = 100\n",
    "# Интервал в эпохах для сохранения сгенерированных изображений в процессе обучения.\n",
    "SAVE_PATH = './models/vae_model_V2.pth'\n",
    "# Путь для сохранения финальной версии обученной модели.\n",
    "OLD_MODEL = False\n",
    "# Путь к ранее обученной модели для продолжения обучения или генерации изображений.\n",
    "# False означает обучение \\\"с нуля\\\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8be2ccbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Folders\n",
    "OUTPUT = './output'\n",
    "EPOCHS_FOLDER = os.path.join(OUTPUT, 'epochs')\n",
    "GENERATED_FOLDER = os.path.join(OUTPUT, 'generated')\n",
    "MORPH_FOLDER = os.path.join(OUTPUT, 'morph_frames')\n",
    "Z_FOLDER = os.path.join(OUTPUT, 'z_frames')\n",
    "LAT_GRID = os.path.join(OUTPUT, 'latent_frames')\n",
    "GRID_FOLDER = os.path.join(OUTPUT, 'grid')\n",
    "os.makedirs(OUTPUT, exist_ok=True)\n",
    "os.makedirs(EPOCHS_FOLDER, exist_ok=True)\n",
    "os.makedirs(GENERATED_FOLDER, exist_ok=True)\n",
    "os.makedirs(MORPH_FOLDER, exist_ok=True)\n",
    "os.makedirs(Z_FOLDER, exist_ok=True)\n",
    "os.makedirs(LAT_GRID, exist_ok=True)\n",
    "os.makedirs(GRID_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a98e16d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using DEVICE: cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using DEVICE: {DEVICE}\")\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e13edddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подготовка данных\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fabaf893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание загрузчиков данных\n",
    "num_workers = min(16, multiprocessing.cpu_count())\n",
    "prefetch_factor = 128 * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e496c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1800 images from 'data'\n"
     ]
    }
   ],
   "source": [
    "dataset = datasets.ImageFolder(root='data', transform=transform)\n",
    "#dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,           # обязательно перемешивание\n",
    "    num_workers=num_workers,           # или другое оптимальное число\n",
    "    pin_memory=True,\n",
    "    prefetch_factor=prefetch_factor,       # если много CPU\n",
    "    persistent_workers=True  # если поддерживается версией\n",
    ")\n",
    "\n",
    "print(f\"Loaded {len(dataset)} images from 'data'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d19a8f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализируем Perceptual Loss\n",
    "#perceptual_loss_fn = VGGPerceptualLoss().to(DEVICE)\n",
    "\n",
    "#def vae_loss_function(recon_x, x, mu, logvar, alpha=1.0, beta=0.1):\n",
    "   # recon_loss = nn.functional.mse_loss(recon_x, x, reduction='sum')\n",
    " #   perceptual_loss = perceptual_loss_fn(recon_x, x) * x.size(0)  # Масштабируем под батч\n",
    "  #  kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    \n",
    "   # total_loss = alpha * recon_loss + beta * perceptual_loss + kl_loss\n",
    "    #return total_loss\n",
    "\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    recon_loss = nn.functional.mse_loss(recon_x, x, reduction='sum')\n",
    "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return recon_loss + kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d065a6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вспомогательные функции\n",
    "def plot_loss(losses):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(losses, label='Training Loss')\n",
    "    plt.xlabel('Эпоха')\n",
    "    plt.ylabel('Потери')\n",
    "    plt.title('График изменения потерь')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def imshow(img):\n",
    "    img = np.array(img)\n",
    "    if img.min() < 0:\n",
    "        img -= img.min()\n",
    "        img /= img.max()\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(np.transpose(img, (1, 2, 0)))\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3cb4f93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализация модели\n",
    "vae = VAE(LATENT_DIM, IMAGE_SIZE).to(DEVICE)\n",
    "perceptual_loss_fn = VGGPerceptualLoss(device=DEVICE).to(DEVICE)\n",
    "if OLD_MODEL:\n",
    "    vae.load_state_dict(torch.load(OLD_MODEL, map_location=DEVICE))\n",
    "    print(\"Loaded model from\", OLD_MODEL)\n",
    "optimizer = optim.Adam(vae.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a952c703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучение модели\n",
    "losses = []\n",
    "best_loss = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26205e3b",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 512.00 MiB. GPU 0 has a total capacity of 22.41 GiB of which 492.63 MiB is free. Of the allocated memory 15.80 GiB is allocated by PyTorch, and 612.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m recon_imgs, mu, logvar \u001b[38;5;241m=\u001b[39m vae(imgs)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Перцептуальная + KL-дивергенция\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m perceptual \u001b[38;5;241m=\u001b[39m \u001b[43mperceptual_loss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecon_imgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m kl \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m logvar \u001b[38;5;241m-\u001b[39m mu\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m-\u001b[39m logvar\u001b[38;5;241m.\u001b[39mexp())\n\u001b[0;32m     18\u001b[0m loss \u001b[38;5;241m=\u001b[39m perceptual \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.05\u001b[39m \u001b[38;5;241m*\u001b[39m kl  \u001b[38;5;66;03m# веса можно регулировать\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\teacher\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\teacher\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\AI\\AvtoEncoder\\VAE\\vae_v2.py:119\u001b[0m, in \u001b[0;36mVGGPerceptualLoss.forward\u001b[1;34m(self, x, y)\u001b[0m\n\u001b[0;32m    117\u001b[0m     x \u001b[38;5;241m=\u001b[39m block(x)\n\u001b[0;32m    118\u001b[0m     y \u001b[38;5;241m=\u001b[39m block(y)\n\u001b[1;32m--> 119\u001b[0m     loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\teacher\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\functional.py:3905\u001b[0m, in \u001b[0;36mmse_loss\u001b[1;34m(input, target, size_average, reduce, reduction, weight)\u001b[0m\n\u001b[0;32m   3901\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   3902\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid reduction mode: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreduction\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Expected one of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3903\u001b[0m         )\n\u001b[0;32m   3904\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3905\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3906\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpanded_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpanded_target\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3907\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 0 has a total capacity of 22.41 GiB of which 492.63 MiB is free. Of the allocated memory 15.80 GiB is allocated by PyTorch, and 612.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    tstart = datetime.datetime.now()\n",
    "    vae.train()\n",
    "    train_loss = 0\n",
    "    recon_loss_total = 0\n",
    "    kl_loss_total = 0\n",
    "\n",
    "    for imgs, _ in dataloader:\n",
    "        imgs = imgs.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        recon_imgs, mu, logvar = vae(imgs)\n",
    "\n",
    "        # Перцептуальная + KL-дивергенция\n",
    "        perceptual = perceptual_loss_fn(recon_imgs, imgs)\n",
    "        kl = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "        loss = perceptual + 0.05 * kl  # веса можно регулировать\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        recon_loss_total += perceptual.item()\n",
    "        kl_loss_total += kl.item()\n",
    "\n",
    "    avg_loss = train_loss / len(dataloader)\n",
    "    avg_recon_loss = recon_loss_total / len(dataloader)\n",
    "    avg_kl_loss = kl_loss_total / len(dataloader)\n",
    "\n",
    "    losses.append(avg_loss)\n",
    "\n",
    "    print(f\"\\nEpoch [{epoch + 1}/{EPOCHS}]\")\n",
    "    print(f\"Train Loss: {avg_loss:.4f} (Recon: {avg_recon_loss:.4f}, KL: {avg_kl_loss:.4f})\")\n",
    "    print(f\"Время: {str(datetime.datetime.now() - tstart).split('.')[0]}\")\n",
    "\n",
    "    # Сохраняем лучшую модель\n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        best_model = vae\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"val_loss: {avg_loss:.4f} > best_loss: {best_loss:.4f}, попытка {patience_counter}\")\n",
    "        if patience_counter >= EARLY_STOP:\n",
    "            print(\"Ранняя остановка!\")\n",
    "            break\n",
    "\n",
    "    # Периодическая генерация\n",
    "    if (epoch + 1) % SAVE_INTERVAL == 0:\n",
    "        vae.eval()\n",
    "        with torch.no_grad():\n",
    "            z = torch.randn(4, LATENT_DIM).to(DEVICE)\n",
    "            samples = vae.decoder(z)\n",
    "        vutils.save_image(samples.cpu(), f'{EPOCHS_FOLDER}/generated_faces_epoch_{epoch+1}.png', nrow=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac90b33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BESTLoss = 250\n",
    "best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4982ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохранение финальной модели\n",
    "if best_loss <= BESTLoss:\n",
    "    torch.save({\n",
    "        \"model\": best_model.state_dict(),\n",
    "        \"BATCH_SIZE\": BATCH_SIZE,\n",
    "        \"LATENT_DIM\":LATENT_DIM,\n",
    "        \"IMAGE_SIZE\":IMAGE_SIZE,\n",
    "        \"v\":2\n",
    "    }, SAVE_PATH)\n",
    "\n",
    "    print(f\"Final model saved to {SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30de4091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Генерация итоговых лиц\n",
    "vae.eval()\n",
    "with torch.no_grad():\n",
    "    z = torch.randn(16, LATENT_DIM).to(DEVICE)\n",
    "    samples = vae.decoder(z)\n",
    "vutils.save_image(samples.cpu(), f'{GENERATED_FOLDER}/generated_faces_final.png', nrow=4)\n",
    "imshow(torchvision.utils.make_grid(samples.cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb82ced1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Построение графика потерь\n",
    "plot_loss(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46831f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Уведомление о завершении обучения\n",
    "url = \"http://home.teyhd.ru:3334/\"\n",
    "params = {\"msg\": f\"Обучение Автоэнкодера завершено: Эпох {EPOCHS}\\nФинальный лосс: {losses[-1]:.6f}\"}\n",
    "try:\n",
    "    response = requests.get(url, params=params)\n",
    "    print(f\"Notification sent! Status Code: {response.status_code} | Response Text: {response.text}\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to send notification: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12a4162",
   "metadata": {},
   "source": [
    "### Генерация разных типов z для декодера и создание GIF морфинга"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39aa048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Настройки ---\n",
    "NUM_PIC = 16  # Количество изображений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e35ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Стандартное сэмплирование (N(0,1)) ---\n",
    "with torch.no_grad():\n",
    "    z = torch.randn(NUM_PIC, LATENT_DIM).to(DEVICE)\n",
    "    samples = vae.decoder(z)\n",
    "    vutils.save_image(samples.cpu(), f'{Z_FOLDER}/z_random_normal.png', nrow=4)\n",
    "    imshow(torchvision.utils.make_grid(samples.cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b879f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Ручное задание признаков ---\n",
    "with torch.no_grad():\n",
    "    z = torch.zeros(NUM_PIC, LATENT_DIM).to(DEVICE)\n",
    "    z[:, :2] = torch.linspace(-3, 3, NUM_PIC).unsqueeze(1).repeat(1, 2)\n",
    "    samples = vae.decoder(z)\n",
    "    vutils.save_image(samples.cpu(), f'{Z_FOLDER}/z_manual_features.png', nrow=4)\n",
    "    imshow(torchvision.utils.make_grid(samples.cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7489685a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Латентные коды реальных изображений ---\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe195ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_dataset = datasets.ImageFolder(root='data', transform=transform)\n",
    "real_loader = DataLoader(real_dataset, batch_size=NUM_PIC, shuffle=True)\n",
    "\n",
    "real_imgs, _ = next(iter(real_loader))\n",
    "real_imgs = real_imgs.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d56475",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    mu, logvar = vae.encoder(real_imgs)\n",
    "    z = vae.reparameterize(mu, logvar)\n",
    "    samples = vae.decoder(z)\n",
    "    vutils.save_image(samples.cpu(), f'{Z_FOLDER}/z_from_real_imgs.png', nrow=4)\n",
    "    imshow(torchvision.utils.make_grid(samples.cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a38f870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Интерполяция между двумя точками ---\n",
    "frames = []\n",
    "with torch.no_grad():\n",
    "    z1 = torch.randn(1, LATENT_DIM).to(DEVICE)\n",
    "    z2 = torch.randn(1, LATENT_DIM).to(DEVICE)\n",
    "    alphas = torch.linspace(0, 1, steps=30).unsqueeze(1).to(DEVICE)\n",
    "    z = (1 - alphas) * z1 + alphas * z2\n",
    "    for i, zi in enumerate(z):\n",
    "        sample = vae.decoder(zi.unsqueeze(0))\n",
    "        img_path = f'{MORPH_FOLDER}/frame_{i:03d}.png'\n",
    "        vutils.save_image(sample.cpu(), img_path, nrow=1)\n",
    "        frames.append(imageio.imread(img_path))\n",
    "    imageio.mimsave(f'{MORPH_FOLDER}/morphing.gif', frames, fps=10)\n",
    "    print(\"GIF морфинга сохранён как 'morphing.gif'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd05ebf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Показываем последний батч интерполяции\n",
    "vutils.save_image(vae.decoder(z).cpu(), f'{Z_FOLDER}/z_interpolation.png', nrow=6)\n",
    "imshow(torchvision.utils.make_grid(vae.decoder(z).cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5b0292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Альтернативное распределение (равномерное) ---\n",
    "with torch.no_grad():\n",
    "    z = torch.empty(NUM_PIC, LATENT_DIM).uniform_(-2, 2).to(DEVICE)\n",
    "    samples = vae.decoder(z)\n",
    "    vutils.save_image(samples.cpu(), f'{Z_FOLDER}/z_uniform_distribution.png', nrow=4)\n",
    "    imshow(torchvision.utils.make_grid(samples.cpu()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2625482a",
   "metadata": {},
   "source": [
    "### Код для удобного подбора ручных признаков и генерации 2D-сетки лиц + анимация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b4bad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Настройки ---\n",
    "GRID_SIZE = 8  # Сетка GRID_SIZE x GRID_SIZE\n",
    "# Предполагаем, что модель VAE уже загружена и называется \"vae\"\n",
    "vae.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262e04bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Удобный подбор параметров ---\n",
    "def generate_manual_z(fixed_value=0.0, vary_dims=(0, 1), range_min=-3, range_max=3, grid_size=GRID_SIZE):\n",
    "    total = grid_size * grid_size\n",
    "    z = torch.full((total, LATENT_DIM), fixed_value).to(DEVICE)\n",
    "    values = torch.linspace(range_min, range_max, steps=grid_size)\n",
    "    grid = torch.cartesian_prod(values, values)\n",
    "    for idx, (val_x, val_y) in enumerate(grid):\n",
    "        z[idx, vary_dims[0]] = val_x\n",
    "        z[idx, vary_dims[1]] = val_y\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a498b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Генерация 2D-сетки лиц ---\n",
    "def generate_face_grid(z, save_path=f'{GRID_FOLDER}/grid_faces.png', grid_size=GRID_SIZE):\n",
    "    with torch.no_grad():\n",
    "        samples = vae.decoder(z)\n",
    "    vutils.save_image(samples.cpu(), save_path, nrow=grid_size)\n",
    "    print(f\"Сетка лиц сохранена в {save_path}\")\n",
    "    imshow(torchvision.utils.make_grid(samples.cpu(), nrow=grid_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a3ed4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Анимация перемещения по латентному пространству ---\n",
    "def generate_latent_animation(vary_dims=(0, 1), steps=30, fixed_value=0.0, range_min=-3, range_max=3, save_path='{OUTPUT}/latent_walk.gif'):\n",
    "    frames = []\n",
    "  \n",
    "    with torch.no_grad():\n",
    "        for step in range(steps):\n",
    "            alpha = step / (steps - 1)\n",
    "            val_x = (1 - alpha) * range_min + alpha * range_max\n",
    "            val_y = np.sin(alpha * np.pi * 2) * (range_max / 2)\n",
    "\n",
    "            z = torch.full((1, LATENT_DIM), fixed_value).to(DEVICE)\n",
    "            z[0, vary_dims[0]] = val_x\n",
    "            z[0, vary_dims[1]] = val_y\n",
    "\n",
    "            sample = vae.decoder(z)\n",
    "            img_path = f'{LAT_GRID}/frame_{step:03d}.png'\n",
    "            vutils.save_image(sample.cpu(), img_path, nrow=1)\n",
    "            frames.append(imageio.imread(img_path))\n",
    "\n",
    "    imageio.mimsave(save_path, frames, fps=10)\n",
    "    print(f\"Анимация латентного перехода сохранена в {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf1af16",
   "metadata": {},
   "source": [
    "### Примеры использования"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3184ea2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Меняем 0-й и 1-й признак\n",
    "z = generate_manual_z(fixed_value=0.0, vary_dims=(0, 1), range_min=-3, range_max=3)\n",
    "generate_face_grid(z, save_path=f'{GRID_FOLDER}/grid_faces_0_1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a401ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Меняем 2-й и 3-й признак\n",
    "z = generate_manual_z(fixed_value=0.0, vary_dims=(2, 3), range_min=-2, range_max=2)\n",
    "generate_face_grid(z, save_path=f'{GRID_FOLDER}/grid_faces_2_3.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be08345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Узкий диапазон изменений для тонких вариаций\n",
    "z = generate_manual_z(fixed_value=0.0, vary_dims=(5, 6), range_min=-1, range_max=1)\n",
    "generate_face_grid(z, save_path=f'{GRID_FOLDER}/grid_faces_5_6.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9671198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Генерация анимации перемещения по латентному пространству\n",
    "generate_latent_animation(vary_dims=(0, 1), steps=40, save_path=f'{OUTPUT}/latent_walk.gif')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
